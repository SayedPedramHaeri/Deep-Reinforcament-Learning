{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e34d1ec",
   "metadata": {},
   "source": [
    "# Project: Deep Reinforcement Learning - Value and Policy Iterations\n",
    "**Author:** Sayed Pedram Haeri Boroujeni  \n",
    "**Position:** PhD Student, Clemson University  \n",
    "**Affiliation:** Department of Computer Science  \n",
    "**Email:** shaerib@g.clemson.edu  \n",
    "**Date Created:** June 10, 2025  \n",
    "**Last Updated:** June 15, 2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637ce41-0590-425a-af2b-12c70e2259a1",
   "metadata": {},
   "source": [
    "##### 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bef2d75-94e4-4a73-8bde-d2215148683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5417e-6532-44bc-8fc4-ad640a1a40b3",
   "metadata": {},
   "source": [
    "##### 2. Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c217eb-dbf7-4bbb-906d-dd9ec1b685de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ff3c3-e5ac-4f15-9d3c-b7cd8b8a0538",
   "metadata": {},
   "source": [
    "##### 3. Defining FrozenLake Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c09d48a7-d3a0-407d-b53e-e251c4e26a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Discrete(16)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# If map_name is specified (e.g., \"4x4\" or \"8x8\"), the environment uses a predefined map of that size.\n",
    "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", render_mode=\"human\")\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6319ff-8ab0-40ba-b142-3cf834b07b6c",
   "metadata": {},
   "source": [
    "##### 4. Inspecting Terminal and Goal States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b650c3-10be-4475-905b-532c330d06f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of States: 16\n",
      "Number of Actions: 4\n",
      "\n",
      "FrozenLake Grid (S=start, F=frozen, H=hole, G=goal):\n",
      "S F F F\n",
      "F H F H\n",
      "F F F H\n",
      "H F F G\n"
     ]
    }
   ],
   "source": [
    "nS = env.observation_space.n   # number of states\n",
    "nA = env.action_space.n        # number of actions\n",
    "print(\"Number of States:\", nS)\n",
    "print(\"Number of Actions:\", nA)\n",
    "\n",
    "desc = env.unwrapped.desc.astype(\"U\")\n",
    "print(\"\\nFrozenLake Grid (S=start, F=frozen, H=hole, G=goal):\")\n",
    "for row in desc:\n",
    "    print(\" \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bab862c-7153-4b85-8dbf-87ab71e2dabf",
   "metadata": {},
   "source": [
    "##### 5. Generating a Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b1242b-b7cd-4049-b381-ec99464da4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Policy (probabilities for each action in each state):\n",
      " State  0: [0.25 0.25 0.25 0.25]\n",
      " State  1: [0.25 0.25 0.25 0.25]\n",
      " State  2: [0.25 0.25 0.25 0.25]\n",
      " State  3: [0.25 0.25 0.25 0.25]\n",
      " State  4: [0.25 0.25 0.25 0.25]\n",
      " State  5: [0.25 0.25 0.25 0.25]\n",
      " State  6: [0.25 0.25 0.25 0.25]\n",
      " State  7: [0.25 0.25 0.25 0.25]\n",
      " State  8: [0.25 0.25 0.25 0.25]\n",
      " State  9: [0.25 0.25 0.25 0.25]\n",
      " State 10: [0.25 0.25 0.25 0.25]\n",
      " State 11: [0.25 0.25 0.25 0.25]\n",
      " State 12: [0.25 0.25 0.25 0.25]\n",
      " State 13: [0.25 0.25 0.25 0.25]\n",
      " State 14: [0.25 0.25 0.25 0.25]\n",
      " State 15: [0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "random_policy = np.ones((nS, nA)) / nA\n",
    "print(\"Random Policy (probabilities for each action in each state):\")\n",
    "for i in range(nS):\n",
    "    print(f\" State {i:2d}: {random_policy[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b30a4f-1359-4d08-890f-efaa7aa254c6",
   "metadata": {},
   "source": [
    "##### 6. Defining Policy Eavluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "353ee733-466e-4be5-b5d6-30d663521a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=0.99, theta=1e-10):\n",
    "    \"\"\"\n",
    "        Evaluates a given stochastic policy for a stichastic environment.\n",
    "    Args:\n",
    "        env: The OpenAI Gym environment (must have .P, .nS, .nA).\n",
    "        policy: A 2D NumPy array of shape (nS, nA), where each row is a probability distribution over actions.\n",
    "        gamma: Discount factor for future rewards.\n",
    "        theta: Convergence threshold â€” stops iteration when value change is below this amount.\n",
    "    Returns:\n",
    "        V: Value function for each state\n",
    "        num_iterations: Number of iterations taken to converge\n",
    "    \"\"\"\n",
    "    prev_V = np.zeros(nS)        # Initialize previous value function to zeros   \n",
    "    eval_iters = 0\n",
    "    \n",
    "    while True:\n",
    "        eval_iters +=1\n",
    "        delta = 0           # Change in value function                \n",
    "        V = np.zeros(nS)    # Initialize current value function to zeros\n",
    "        \n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    V[state] += policy[state][action] * prob * (reward + gamma * prev_V[next_state])   \n",
    "                    \n",
    "        delta = np.max(np.abs(V - prev_V))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "        prev_V = V\n",
    "\n",
    "    return V, eval_iters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d871e9a8-783a-4bd1-85f9-84ec45778ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Evaluation converged after 93 iterations.\n",
      "V Values is:\n",
      "0.0124  0.0104  0.0193  0.0095\n",
      "0.0148  0.0000  0.0389  0.0000\n",
      "0.0326  0.0843  0.1378  0.0000\n",
      "0.0000  0.1703  0.4336  0.0000\n"
     ]
    }
   ],
   "source": [
    "V, eval_iters= policy_evaluation(env, random_policy)\n",
    "print(f\"Policy Evaluation converged after {eval_iters} iterations.\")\n",
    "print(\"V Values is:\")\n",
    "\n",
    "# Automatically compute grid size (assumes square layout)\n",
    "grid_size = int(np.sqrt(len(V)))\n",
    "V_grid = V.reshape((grid_size, grid_size))\n",
    "for row in V_grid:\n",
    "    print(\"  \".join(f\"{val:.4f}\" for val in row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d3c40b-80c2-4e04-9943-42fcbdffbfa1",
   "metadata": {},
   "source": [
    "##### 7. Defining Policy Improvement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c068f0-e613-48a7-aff3-c89fa2c7daa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=0.99):\n",
    "    \"\"\"\n",
    "        Performs a greedy policy improvement step based on the given value function V.\n",
    "    Args:\n",
    "        env: The OpenAI Gym or Gymnasium environment (must have .P)\n",
    "        V: A 1D NumPy array of shape (nS,) representing the state value function\n",
    "        gamma: Discount factor\n",
    "    Returns:\n",
    "        policy: A 2D NumPy array of shape (nS, nA) representing the improved policy (one-hot greedy)\n",
    "    \"\"\"\n",
    "    policy = np.zeros((nS, nA))\n",
    "    \n",
    "    for state in range(nS):\n",
    "        q_values = np.zeros(nA)\n",
    "        for action in range(nA):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                q_values[action] += prob * (reward + gamma * V[next_state])\n",
    "                \n",
    "        best_action_index = np.argmax(q_values)\n",
    "        policy[state][best_action_index] = 1.0  # Greedy one-hot policy\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc17ae37-838a-46bc-a0b1-7a388370efc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "improved_policy = policy_improvement(env, V)\n",
    "improved_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a2540-3f1f-4709-8891-f3f955ce27dc",
   "metadata": {},
   "source": [
    "##### 8. Defining Policy Iteration Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23061eb5-a301-4f85-bd6e-583ba1624f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=0.99, theta=1e-10):\n",
    "    \"\"\"\n",
    "        Performs policy iteration for a given environment.\n",
    "    Returns:\n",
    "        final_policy: optimal policy (nS x nA)\n",
    "        final_value: optimal value function (nS,)\n",
    "        iterations: number of improvement iterations performed\n",
    "    \"\"\"\n",
    "    policy = random_policy # Start with uniform random policy\n",
    "    policy_iters = 0\n",
    "\n",
    "    while True:\n",
    "        V, eval_iters = policy_evaluation(env, policy)\n",
    "        new_policy = policy_improvement(env, V)\n",
    "        policy_iters += 1\n",
    "        \n",
    "        # Check if policy is stable\n",
    "        if np.array_equal(new_policy, policy):\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "    \n",
    "    return policy, V, policy_iters, eval_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f326481-8037-4a4c-86b3-a28ed5257f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration converged in 3 iterations using 574 total policy evaluation updates.\n",
      "Optimal Policy using policy iteration method is: \n",
      " [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "pi_optimal_policy, optimal_value, policy_iters, eval_iters = policy_iteration(env)\n",
    "print(f\"Policy Iteration converged in {policy_iters} iterations using {eval_iters} total policy evaluation updates.\")\n",
    "print(\"Optimal Policy using policy iteration method is: \\n\", pi_optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4013df70-4826-4bbf-8369-1aba4c2d42af",
   "metadata": {},
   "source": [
    "##### 9. Defining Value Iteration Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a55483-769b-4f31-b81f-7bb7f03b4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-10):\n",
    "    \"\"\"\n",
    "        Performs value iteration to compute the optimal policy and value function.\n",
    "    Args:\n",
    "        env: OpenAI Gym environment (must have .P, .nS, .nA)\n",
    "        gamma: Discount factor\n",
    "        theta: Convergence threshold\n",
    "    Returns:\n",
    "        V: Optimal value function\n",
    "        policy: Optimal policy (one-hot, shape nS x nA)\n",
    "        value_iters: Number of iterations until convergence\n",
    "    \"\"\"\n",
    "    policy = np.zeros((nS, nA))\n",
    "    prev_V = np.zeros(nS)\n",
    "    value_iters = 0\n",
    "\n",
    "    while True:\n",
    "        value_iters += 1\n",
    "        q_values = np.zeros((nS, nA))\n",
    "\n",
    "        for state in range(nS):\n",
    "            for action in range(nA):\n",
    "                for prob, next_state, reward, done in env.P[state][action]:\n",
    "                    q_values[state][action] += prob * (reward + gamma * prev_V[next_state])\n",
    "            \n",
    "        V = np.max(q_values, axis=1)\n",
    "        delta = np.max(np.abs(prev_V - V))\n",
    "        prev_V = V\n",
    "        \n",
    "        if delta < theta:\n",
    "            break        \n",
    "\n",
    "    for state in range(nS):\n",
    "        best_action = np.argmax(q_values[state])\n",
    "        policy[state][best_action] = 1.0  # Greedy one-hot policy\n",
    "\n",
    "    return policy, value_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c595ba-75c1-431c-8b1b-1512c049c8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration converged in 571 iterations.\n",
      "Optimal Policy using value iteration method is: \n",
      " [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "vi_optimal_policy, value_iters = value_iteration(env)\n",
    "print(f\"Value Iteration converged in {value_iters} iterations.\")\n",
    "print(\"Optimal Policy using value iteration method is: \\n\", pi_optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723ab0da-b11f-4e2e-80bc-f8ca28c4b03c",
   "metadata": {},
   "source": [
    "##### 10. Comparing Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad245e5c-de12-4678-991a-96f6b0e23b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration took 0.26 Seconds.\n",
      "Policy Iteration converged in 3 iterations.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "pi_optimal_policy, optimal_value, policy_iters, eval_iters = policy_iteration(env)\n",
    "end = time.time()\n",
    "print(f\"Policy Iteration took {end-start:.2f} Seconds.\")\n",
    "print(f\"Policy Iteration converged in {policy_iters} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c313d37f-8da7-4d92-80ac-2b4cfb7c164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration took 0.11 Seconds.\n",
      "Value Iteration converged in 571 iterations.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "vi_optimal_policy, value_iters = value_iteration(env)\n",
    "end = time.time()\n",
    "print(f\"Value Iteration took {end-start:.2f} Seconds.\")\n",
    "print(f\"Value Iteration converged in {value_iters} iterations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb11f36-e71a-49ee-ab8f-d7b15c999a13",
   "metadata": {},
   "source": [
    "##### 11. Extra-information - Showing in PyGame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fd8612-b500-4f88-a233-120e58c4dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "np.bool8 = np.bool_\n",
    "# Simulate one episode using the value-based greedy policy\n",
    "def run_episode(env, V, gamma=0.99, delay=0.5):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        time.sleep(delay)\n",
    "        env.render()\n",
    "\n",
    "        # Greedy action based on estimated values\n",
    "        best_action = None\n",
    "        best_value = -float(\"inf\")\n",
    "        for action in range(env.action_space.n):\n",
    "            value = sum(\n",
    "                prob * (reward + gamma * V[next_state])\n",
    "                for prob, next_state, reward, done in env.P[state][action]\n",
    "            )\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "\n",
    "        state, reward, done, _, _ = env.step(best_action)\n",
    "        total_reward += reward\n",
    "\n",
    "    env.render()\n",
    "    print(f\"Episode finished. Total reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b13bdfc-5ff2-40a8-a0ad-429b20579bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_episode(env, V)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
